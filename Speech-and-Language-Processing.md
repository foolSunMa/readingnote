# 自然语言综述读书笔记
### 原始链接：https://web.stanford.edu/~jurafsky/slp3/
## 第十章
#### 关键词：
##### 预训练(pretrain):
##### 大语言模型(Large language model):
##### 转换器(transformer):
##### 生成式AI(genrative AI):
##### 贪心解码策略(greedy decoding):
##### 采样(sampling):
##### 摘要任务(summarization):
### 10.1 基于transformers的大语言模型
### 10.2 大模型生成任务中的采样问题
#### 10.2.1 Top-k采样
#### 10.2.2 核采样/Top-p采样
#### 10.2.3 温度采样
### 10.3 预训练大语言模型
#### 10.3.1 自监督训练算法
#### 10.3.2 大语言模型训练预料
#### 10.3.3 微调
### 10.4 评估大语言模型
#### 困惑度(Perplexity)
#### 其他指标
### 10.5 处理规模
#### 10.5.1 Scaling laws
#### 10.5.2 KV Cache
#### 10.5.3 参数效率微调
### 10.6 语言模型的潜在伤害
### 10.7 总结
## 第十一章

## 第十二章
